---
title: "clustering"
author: "Bobby Yang"
date: "10/3/2022"
output: html_document
---

#Processing
```{r}
cut.val = .8
#single cell
cut.val = .99
#truth

library(parallel)
library(tidyverse)

# This will set your directory to where this R script is stored.
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

pwd = getwd()

#pattern = 'SINGLE_CELL', 'TRUTH'
files.high = list.files(path = "700-900 single", pattern = "SINGLE_CELL", full.names = T)
files.low = list.files(path = "250-400 single", pattern = "SINGLE_CELL", full.names = T)

files = c(files.high, files.low)
length(files)

# This will tell you how many cores your computer has
numCores <- detectCores()

# This initiates a (blank?) cluster object.
cl = makeCluster(numCores)


# This is where you upload any variables needed before running the parallelization.
clusterExport(cl, c("files", "pwd", "cut.val"))


# This is where you add any code needed before running the parallelization
clusterEvalQ(cl, {
  setwd(pwd)
  # source("./Functions_for_single_cell_data_6_27.R")
  # This requires a vector of cell labels to partition the data into clone datasets.
  Make.clones.from.sc.data = function(dat, cell.status){
    clones = list()
    clone.types = 1:length(unique(cell.status))
    for(i in 1:length(unique(cell.status))){
      clones[[i]] = dat[, which(cell.status==as.character(clone.types[i]-1))]
      # clones[[i]] = non.singletons.dat[, which(cell.status==as.character(clone.types[i]-1))]
    }
    
    return(clones)
  }
  
  
  remove.singletons.from.data = function(dat){
    
    
    # Remove the singletons; make a dataset without the singletons.
    singletons = c()
    count = 1
    
    temp.rows = rowSums(dat, na.rm = T)
    
    for(i in 1:dim(dat)[1]){
      if(temp.rows[i]==1){
        singletons[count] = i
        count = count + 1
      }
    }
    
    if(length(singletons)>0){
      non.singletons.dat = dat[-singletons,]
    }else{
      non.singletons.dat = dat
    }
    
    
    return(non.singletons.dat)
  }
  
}
)

f = function(i){
  
  
  temp.dat = read.csv(files[i])

  # Add the true t.1 here!
  # Add the true t.1 here!
  # Add the true t.1 here!
  
  string <- strsplit(files[i],'/')[[1]][2]
  true.t1 <- substr(string,1,12)
  
  #string <- str_extract(temp.dat,'([^/]*)$')
  #true.t1 = str_extract(string,regex('^.+?(?=_)'))
  temp.dat = temp.dat[, grep(pattern = "Cell", x = colnames(temp.dat))]
  
  
  # temp.dat = new.label.dats[[i]]
  
  # First row has the cell labels.
  temp.labels = as.vector(t(temp.dat[1,grep(pattern = "Cell", x = colnames(temp.dat))]))
  
  temp.dat = remove.singletons.from.data(temp.dat)
  
  
  
  
  
  # Drop cell.labels from the data.
  cleaned.dat = temp.dat[-1,]
  
  # Get the clones.
  clones = Make.clones.from.sc.data(dat = cleaned.dat, cell.status = temp.labels)
  
  # Get Clone 1.
  clone1 = clones[[2]]
  
  # Get the SFS of Clone 1.
  sfs.clone1 = rowSums(clone1, na.rm = T)
  
  # Find the 'selective' mutations of Clone 1.
  K1.mutations = names(sfs.clone1[which(sfs.clone1 >= dim(clone1)[2]*cut.val)])
  
  # Find the neutral mutations of Clone 1
  A1.mutations = names(sfs.clone1[which(sfs.clone1 < dim(clone1)[2]*cut.val & sfs.clone1 > 0)])
  
  # Find the neutral mutations of Clone 0.
  clone0 = clones[[1]]
  
  sfs.clone0 = rowSums(clone0, na.rm = T)
  
  A0.mutations = names(sfs.clone0[which( sfs.clone0 > 0 & sfs.clone1 < dim(clone1)[2]*cut.val)])
  
  # If mutations separated correctly, this should print. Otherwise, coding error on my part...
  if(sum(length(A0.mutations) + length(K1.mutations) + length(A1.mutations))==dim(cleaned.dat)[1]){
    print("Good!")
  }
  
  p0 = dim(clone0)[2]/dim(cleaned.dat)[2]
  p1 = dim(clone1)[2]/dim(cleaned.dat)[2]
  A0 = length(A0.mutations)
  A1 = length(A1.mutations)
  K1 = length(K1.mutations)
  #n <- dim(cleaned.dat)[2]
  output = c(K1, p0, p1, A0, A1, true.t1)
  names(output) = c("K_1", "p_0", "p_1", "A_0", "A_1", "true.t1")
  
  return(output)
}

# This runs the parallel code.
mutation_count_results = parLapply(cl, 1:length(files), f )

# This must ALWAYS be run after running parallel code.
stopCluster(cl)

new.mutation.dats = list()
new.mut.count = 1

for(i in 1:length(mutation_count_results)){
  temp.dat = mutation_count_results[[i]]
  if(length(temp.dat)>1){
    new.mutation.dats[[new.mut.count]] = temp.dat
    new.mut.count = new.mut.count + 1
  }
}

#Create data frame with summary statistics as parameters
results = as.data.frame(matrix(rep(NA, length(files)*6),
                               ncol = 6,
                               nrow = length(files)  ))

colnames(results) = c("K_1", "p_0", "p_1", "A_0", "A_1", "true.t1")

for(i in 1:length(new.mutation.dats)){
  temp.dat = new.mutation.dats[[i]]
  results[i,] = temp.dat
}

#Extract n and tm values from filename string
N = 10^6
n <- str_extract(files,regex('(?<=n_sample_)...'))
n <- as.numeric(n)
results$n <- n
#n = 300

tm <- str_extract(files,regex('(?<=tm1_)....'))
tm <- as.numeric(tm)
results$tm <- tm

#Calculate estimated theta (mutation rate) values
results$theta_0 = as.numeric(results$A_0)*log(N*as.numeric(results$p_0))/(n*1000*as.numeric(results$p_0))

#Assign low/high groups
results$group <- ifelse(results$true.t1 < 700, 1, 0)
results$group <- factor(results$group,c(1,0))
results
```

#Set n
```{r}
#Break data into 5 random groups for cross-validation

#Full data C-V groups
cut_data <- function(x) {
  total_rows <- seq(1,400)
  rows <- list()
  
  #Sample 80 rows, remove from total, repeat until 5 groups obtained
  for (i in seq(1,5)) {
    rows[[i]] <- sample(total_rows,80)
    total_rows <- setdiff(total_rows,rows[[i]])
  }
  x+1
  return(rows)
}

cv_groups <- cut_data(1)

#Small n (n=20) groups
#Take random sample of 20 rows from data
results <- results[sample(seq(400),20,replace=FALSE),]
cut_data <- function(x) {
  total_rows <- seq(1,20)
  rows <- list()
  
  #Sample 4 rows, remove from total, repeat until 5 groups obtained
  for (i in seq(1,5)) {
    rows[[i]] <- sample(total_rows,4)
    total_rows <- setdiff(total_rows,rows[[i]])
  }
  x+1
  return(rows)
}

cv_groups <- cut_data(1)
```

#Collinear predictor
```{r}
#Generate collinear predictor
results$collin <- results$K_1 + rnorm(400,0,7.965533)
cor.test(results$K_1,results$collin,method='pearson')
#Include this predictor in the below code
```

#Heuristic "line" method
```{r}
results$group <- as.factor(results$group)
#Plot values to get an idea of approximately what the cutoff should be
plot(as.numeric(results$K_1)/results$theta_0, results$A_0,col=results$group) + abline(v=200)

#Determine the optimal cut value by iterating through rough interval of values (user defined)
optim_cut <- function (results) {
  correct_vec <- c()
  
  #Loop through each possible cut value
  for (i in seq(100,300,1)) {
    num_correct <- 0
    
    #Loop through each point, increment by 1 if line predicts correctly
    for (j in seq(nrow(results))){
      if ((as.numeric(results$K_1[j])/results$theta_0[j] > i & results$group[j] == 0) | (as.numeric(results$K_1[j])/results$theta_0[j] < i & results$group[j] == 1)) {
      
        num_correct <- num_correct + 1
      
      }
    }
    
    #Store number correct for given cut value in vector
    correct_vec <- c(correct_vec, num_correct)
  }
  
  #Find maximum value in correct vector for optimal cut value
  index <- which(correct_vec == max(correct_vec))[ceiling(length(which(correct_vec == max(correct_vec))) / 2)]
  
  return(100 + index)
}

#Determine how accurate the cut is by testing it against each value
cut_acc <- function(results,y) {
  num_correct <- 0
  
  #Loops through each point, increments by 1 if correct
  for (j in seq(nrow(results))){
      if ((as.numeric(results$K_1[j])/results$theta_0[j] > y & results$group[j] == 0) | (as.numeric(results$K_1[j])/results$theta_0[j] < y & results$group[j] == 1)) {
        num_correct <- num_correct + 1
      
      }
  }
  #Return accuracy rate
  return(num_correct/nrow(results))
}

#Determine optimal cut
optim_cut(results)
#And find its accuracy rate on the whole data
cut_acc(results,220)

#Determine accuracy rates for all 5 groups, average them to get overall accuracy
succ_rates <- c()
for (i in seq(5)) {
  
  #Assign test group of data, the remaining 4 are training
  test_row <- cv_groups[[i]]
  train_row <- c()
  for (j in setdiff(seq(5),i)) {
    train_row <- c(train_row, cv_groups[[j]])
  }
  
  #Determine optimal cut and accuracy rate for each CV group
  test_cut <- optim_cut(results[train_row,])
  succ_rates <- c(succ_rates,cut_acc(results[test_row,],test_cut))
}

#Average accuracy rates for overall CV accuracy
mean(succ_rates)

write.csv(results,'results.csv')
```

#Logistic regression
```{r}
#Convert parameters to numeric
results$K_1 <- as.numeric(results$K_1)
results$theta_0 <- as.numeric(results$theta_0)
results$k1_scale <- results$K_1/results$theta_0
results$A_1 <- as.numeric(results$A_1)
results$a1_scale <- results$A_1/results$theta_0
results$p_0 <- as.numeric(results$p_0)
results$p_1 <- as.numeric(results$p_1)
results$A_0 <- as.numeric(results$A_0)
results$group <- as.factor(results$group)
levels(results$group) <- c(1,0)

for (clear in seq(1)) {
  acc <- c()
  
  #Loop through each CV group and determine accuracy rate for each one
  for (i in seq(5)) {
    
    #Assign train and test sets
    test_row <- cv_groups[[i]]
    train_row <- c()
    for (j in setdiff(seq(5),i)) {
      train_row <- c(train_row, cv_groups[[j]])
    }
    
    #Fit model
    model <- glm(group~ K_1+A_1, family=binomial(link='logit'),data=results[train_row,])
    
    #Compare model predictions and actual values, calculate acc rate
    test_pred <- predict(model,results[test_row,],type='response')
    test_pred <- ifelse(test_pred > .5, 0, 1)
    acc <- c(acc,sum(test_pred == results[test_row,]$group)/80)
    
  }
}

#Run below if n=20
acc <- acc*20

mean(acc)
acc

summary(model)
```

#Logistic regression, ridge penalty
```{r}
library(glmnet)

for (clear in seq(1)) {
  acc <- c()
  for (i in seq(5)) {
    
    #Assign train and test sets
    test_row <- cv_groups[[i]]
    train_row <- c()
    for (j in setdiff(seq(5),i)) {
      train_row <- c(train_row, cv_groups[[j]])
    }
  
    #Assign vector of responses and matrix of predictors
    y <- results[train_row,10]
    x <- matrix(c(results[train_row,1],results[train_row,5]),ncol=2)
    
    #Find optimal value of lambda
    optim_lamb <- cv.glmnet(x,y,family='binomial',alpha=0)$lambda.min
    
    #change desired values of lambda
    low_lamb <- optim_lamb*.001
    high_lamb <- optim_lamb*10
    
    #Fit model
    model <- glmnet(x,y,family='binomial',alpha=0,lambda=low_lamb)
    
    #Assign matrix of predictors for test data
    x <- matrix(c(results[test_row,1],results[test_row,5]),ncol=2)
    
    #Compare model predictions and actual values, calculate acc rate
    test_pred <- predict(model,s=low_lamb,newx=x,type='response')
    test_pred <- ifelse(test_pred > .5, 0, 1)
    acc <- c(acc,sum(test_pred == results[test_row,]$group)/80)
  }
}

#Run below if n=20
acc <- acc*20

mean(acc)
acc
```

#Logistic regression, lasso penalty
```{r}
for (clear in seq(1)) {
  acc <- c()
  for (i in seq(5)) {
    
    #Assign train and test sets
    test_row <- cv_groups[[i]]
    train_row <- c()
    for (j in setdiff(seq(5),i)) {
      train_row <- c(train_row, cv_groups[[j]])
    }
    
    #Assign vector of responses and matrix of predictors
    y <- results[train_row,10]
    x <- matrix(c(results[train_row,1],results[train_row,5]),ncol=2)
  
    #Find optimal value of lambda
    model <- glmnet(x,y,family='binomial',alpha = 1)
    lamb <- cv.glmnet(x,y,family='binomial',alpha=1)$lambda.min
    
    #change desired values of lambda
    low_lamb <- optim_lamb*.001
    high_lamb <- optim_lamb*5
    
    #Fit model
    model <- glmnet(x,y,family='binomial',alpha=1,lambda=high_lamb)
    
    #Assign matrix of predictors for test data
    x <- matrix(c(results[test_row,1],results[test_row,5]),ncol=2)
    
    #Compare model predictions and actual values, calculate acc rate
    test_pred <- predict(model,s=high_lamb,newx=x,type='response')
    test_pred <- ifelse(test_pred > .5, 0, 1)
    acc <- c(acc,sum(test_pred == results[test_row,]$group)/80)
    
  }
}

#Run below if n=20
acc <- acc*20

mean(acc)
acc
```

#Support vector machines
```{r}
library(e1071)
#Formatting
results$group <- as.numeric(results$group)
results$group <- ifelse(results$group==2,0,1)

for (clear in seq(1)) {
  acc <- c()
  for (i in seq(5)) {
    
    #Assign test and train sets
    test_row <- cv_groups[[i]]
    train_row <- c()
    for (j in setdiff(seq(5),i)) {
      train_row <- c(train_row, cv_groups[[j]])
    }
    
    #Assign vector of responses, matrix of predictors
    y <- results[train_row,10]
    x <- matrix(c(results[train_row,1],results[train_row,5]),ncol=2)
    svm_dat <- data.frame(x,y)
    
    #Tune model, use range of cost values
    tune.out <- tune(svm,y ∼ .,data=svm_dat,kernel='linear', ranges=list(cost=c(0.001,0.01,0.1,1,10,100)))
    
    #Determine optimal cost
    optim_svm <- tune.out$best.model
    
    #change cost, kernel
    #svm_model <- svm(y~.,data=svm_dat,kernel='polynomial',cost=100,scale=FALSE)
    
    #Assign matrix of predictors for test data
    x <- matrix(c(results[test_row,1],results[test_row,5]),ncol=2)
    
    #Compare model predictions to actual values, calculate acc rate
    test_pred <- predict(optim_svm,x)
    test_pred <- ifelse(test_pred > .5, 0, 1)
    acc <- c(acc,sum(test_pred == results[test_row,]$group)/80)
  
  }
}

#Run below if n=20
acc <- acc*20

#Plot SVM
svm_model <- svm(y~.,data=svm_dat,kernel='linear',cost=.1,scale=FALSE)
plot(svm_model,svm_dat)

#Acc rate
acc <- 1-acc
mean(acc)
```